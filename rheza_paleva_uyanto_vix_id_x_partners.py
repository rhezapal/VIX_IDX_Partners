# -*- coding: utf-8 -*-
"""Rheza Paleva Uyanto_VIX_ID/X Partners

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FtJxZi4lo2OmYqz-PfLMlc-HCMXs12hL

# Final Project VIX ID/X - Predicting Credit Risk Score by Machine Learning
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount("/content/drive")

"""# Import Dataset"""

df = pd.read_csv ('/content/drive/MyDrive/Rakamin Mini Project/Rakamin VIX ID X/loan_data_2007_2014.csv')

df.info()

list_item = []
for col in df.columns:
    list_item.append([col, df[col].dtype, df[col].isna().sum(), 100*df[col].isna().sum()/len(df[col]), df[col].nunique(), df[col].unique()[:5]])
desc_df = pd.DataFrame(data=list_item, columns='Feature,Data Type,Null, Null (%),Unique,Unique Sample'.split(","))
desc_df

df.head(5)

"""# Data preprocessing

## Dropping Value part 1

**Some features consider to drop :**
- had 100% null value:
`annual_inc_joint`,`dti_joint`,`verification_status_joint`,`open_acc_6m`,
`open_il_6m`,`open_il_12m`,`open_il_24m`,`mths_since_rcnt_il`,`total_bal_il`,`il_util`,`open_rv_12m`,`open_rv_24m`,`max_bal_bc`,`all_util`,`inq_fi`,`total_cu_tl`,`inq_last_12m`
- represent full of unique value (high cardinality) : `Unnamed: 0`,`member_id`,`id`
- unnecessary feature : `url`,`desc`,`zip_code`,`pymnt_plan`
- only had 1 unique value : `policy_code`,`application_type`
- redundant feature qith feature `grade`: `sub_grade`
"""

df1=df.copy()

df1=df1.drop(columns=['annual_inc_joint','dti_joint','verification_status_joint','open_acc_6m',
                    'open_il_6m','open_il_12m','open_il_24m','mths_since_rcnt_il','total_bal_il',
                    'il_util','open_rv_12m','open_rv_24m','max_bal_bc','all_util','inq_fi','total_cu_tl','inq_last_12m'])
#had 100% null value

df1=df1.drop(columns=['Unnamed: 0','member_id','id','url','desc','zip_code','application_type','policy_code','sub_grade','pymnt_plan'])
#full unique value, unnecessary feature, only 1 value, redundant feature

nums=['loan_amnt','funded_amnt','funded_amnt_inv','int_rate','installment','annual_inc','dti','delinq_2yrs','inq_last_6mths',
      'mths_since_last_delinq','mths_since_last_record','open_acc','pub_rec','revol_bal','revol_util','total_acc','out_prncp','out_prncp_inv',
      'total_pymnt','total_pymnt_inv','total_rec_prncp','total_rec_int','total_rec_late_fee','recoveries','collection_recovery_fee',
      'last_pymnt_amnt','collections_12_mths_ex_med','mths_since_last_major_derog','acc_now_delinq','tot_coll_amt',
      'tot_cur_bal','total_rev_hi_lim']
cats=['term','grade','emp_title','emp_length','home_ownership','verification_status','issue_d','loan_status',
      'purpose','title','addr_state','earliest_cr_line','initial_list_status','last_pymnt_d',
      'next_pymnt_d','last_credit_pull_d']

df1[cats].describe().T

df1[nums].describe().T

"""## Checking Null Value"""

total_null = df1.isnull().sum() # mencari null value masing-masing kolom
percent_missing = df1.isnull().sum() * 100/ len(df) # dicari persentase nya
dtypes = [df1[col].dtype for col in df1.columns] # buat kolom dtype = tipe data

df_missing_value = pd.DataFrame({'total_null': total_null, # buat judulnya dan diisi dari 3 varibel diatas
                                'data_type': dtypes,
                                'percent_missing': percent_missing})
df_missing_value.sort_values(['data_type','percent_missing'], ascending = False,inplace = True) # sorting berdasarkan tipe data dan persentase missing
missing_value = df_missing_value[(df_missing_value['percent_missing']>0)].reset_index() #filter yang percentnya diatas 0)
missing_value

"""## Dropping Feature part 2 
dropping feature for **percentage missing value > 20% :**
`next_pymnt_d`, `mths_since_last_record`, `mths_since_last_major_derog`, `mths_since_last_delinq`
"""

df1=df1.drop(columns=['next_pymnt_d','mths_since_last_record','mths_since_last_major_derog','mths_since_last_delinq'])
#missing value > 20%

"""## Cek Duplikat"""

df.duplicated().sum()

"""# Date Time - Feature Engineering (I)

Source : https://www.kaggle.com/code/anshulrao/loan-default-analysis-and-prediction-python

**A charge-off** usually occurs when the creditor has deemed an outstanding debt is uncollectible; this typically follows 180 days or six months of non-payment. In addition, debt payments that fall below the required minimum payment for the period will also be charged off if the debtor does not make up for the shortfall. The creditor crosses off the consumer’s debt as uncollectible and marks it on the consumer’s credit report as a charge-off. [Source](https://www.investopedia.com/terms/c/chargeoff.asp)

**A default** on debt happens when a borrower fails to repay the funds according to the initial agreement. With most consumer loans, this typically involves missing multiple payments for several weeks or months in a row.

Fortunately, lenders usually allow a grace period before penalizing the borrower after missing one payment. The period between missing a loan payment and having the loan default is known as "delinquency." The delinquency period helps you avoid default by giving you extra time to contact your loan servicer and catch up on missed payments. [Source](https://www.valuepenguin.com/loans/what-does-it-mean-to-default-on-a-loan)

## Ekstrak Fitur Durasi Pinjaman
"""

import time
from datetime import date
from datetime import timedelta
df1['year'] = pd.to_datetime(df1['issue_d'], format="%b-%y") #dari issue_d = tahun mulai credit
df1['year_last_payment'] = pd.to_datetime(df1['last_pymnt_d'], format="%b-%y") #dari last_pymnt_d = bulan pembayaran terakhir
df1['duration_loan'] = round(pd.to_numeric((df1['year_last_payment']-df1['year'])/np.timedelta64(1,'M'))) # Dalam bulan, berapa lama sudah creditnya.

df1.duration_loan.value_counts()

"""## Ekstrak Tahun dalam year"""

df1['tahun_pinjaman']=df1['year'].dt.strftime("%Y").astype(int)

df1.tahun_pinjaman.value_counts()

"""## Dropping Feature part 3
because unnecessary feature (date): `earliest_cr_line`, `last_credit_pull_d`, We only use `issue_d`, `last_payment` for duration_loan
"""

df1=df1.drop(columns=['earliest_cr_line','last_credit_pull_d'])
#unnecessary date time feature

"""# Data Visualization - Analisis Univariate

### Purpose
"""

plt.figure(figsize=(6,5))
ax = sns.countplot(y="purpose", data=df1, palette="GnBu_r", 
                   order=df1['purpose'].value_counts().index, 
                   dodge=False
                   )
ax.set(title="Distribution Purpose");

"""### Country"""

import plotly.graph_objects as go

# Getting frequency of loans given to every state.
statewise_loan_freq = df1.groupby(['addr_state'], as_index=False).size()

fig = go.Figure(
    data=go.Choropleth(
        locations=list(statewise_loan_freq['addr_state'].values),
        z = list(statewise_loan_freq['size'].values),
        locationmode = 'USA-states',
        colorscale = 'Blues'
))

fig.update_layout(
    title_text = 'Loans Offered by State',
    geo_scope='usa',
)

fig.show()

"""### Term"""

plt.figure(figsize=(6, 5))
ax = sns.countplot(x='term', data=df1, palette="GnBu_r")

df1.term.describe()

"""### Loan status"""

plt.figure(figsize=(6, 4))
ax = sns.countplot(y='loan_status', order=df1['loan_status'].value_counts().index,palette="GnBu_r",  
                   dodge=False, data=df1)

df1_purpose = df1.groupby('loan_status').agg({'purpose':'count'}).sort_values('purpose',ascending = False)
df1_purpose=df1_purpose.rename(columns ={'purpose':'total'}).reset_index()
df1_purpose['%'] = round(df1_purpose['total']*100/sum(df1_purpose['total']),2)
df1_purpose

df1.loan_status.describe()

"""### Home Ownership"""

plt.figure(figsize=(6, 5))
ax = sns.countplot(x='home_ownership', order=df1['home_ownership'].value_counts().index, palette="GnBu_r",  data=df1)

"""### Lama Bekerja"""

plt.figure(figsize=(15, 5))
ax = sns.countplot(x="emp_length", order =['< 1 year','1 year','2 years','3 years','4 years','5 years','6 years','7 years','8 years',
                                                      '9 years','10+ years'], palette='GnBu', data=df1)

df1.home_ownership.describe()

"""### Title"""

df1.emp_title.describe()

"""### Debt to Income (DTI)

**The debt-to-income (DTI)** ratio is the percentage of your gross monthly income that goes to paying your monthly debt payments and is used by lenders to determine your borrowing risk.

As a general guideline, 43% is the highest DTI ratio a borrower can have and still get qualified for a mortgage. Ideally, lenders prefer a debt-to-income ratio lower than 36%, with no more than 28% of that debt going towards servicing a mortgage or rent payment. [Source](https://www.investopedia.com/terms/d/dti.asp)
"""

plt.figure(figsize=(15, 8))
ax = sns.histplot(x="dti", data=df1)

df1.dti.describe()

"""### Annual Income"""

df1.annual_inc.describe()

plt.figure(figsize=(10, 2))
ax = sns.boxplot(x="annual_inc", data=df1) # dalam bulan

"""### Durasi Pinjaman"""

df1.duration_loan.describe()

plt.figure(figsize=(10, 2))
ax = sns.boxplot(x="duration_loan", data=df1) # dalam bulan

"""### Biaya Angsuran / Installment"""

df1['installment'].describe()

plt.figure(figsize=(10, 2))
ax = sns.histplot(x="installment", data=df1)

plt.figure(figsize=(10, 2))
ax = sns.boxplot(x="installment", data=df1) # dalam bulan

"""### Jumlah Pinjaman"""

df1['loan_amnt'].describe()

plt.figure(figsize=(10, 2))
ax = sns.histplot(x="loan_amnt", data=df1)

plt.figure(figsize=(10, 2))
ax = sns.boxplot(x="loan_amnt", data=df1)

"""### 9. Interest Rate"""

df1['int_rate'].describe()

plt.figure(figsize=(10, 2))
ax = sns.histplot(x="int_rate", data=df1)

plt.figure(figsize=(10, 2))
ax = sns.boxplot(x="int_rate", data=df1)

"""# Feature Engineering (II)

Feature Engineering :
- `loan_status` -> Risk
Good = Current, Fully Paid, In Grace Periode
Bad = beside Good

## Risk
"""

df1.loan_status.value_counts()

df1['risk']=np.where((df1['loan_status']=='Charged Off') |
                     (df1['loan_status']=='Late (31-120 days)') |
                     (df1['loan_status']=='Late (16-30 days)') |
                     (df1['loan_status']=='Default') |
                     (df1['loan_status']=='Does not meet the credit policy. Status:Charged Off'), 'Bad Risk','Good Risk')

"""- Make a binary clasification : Bad Risk = 1, Good Risk = 0 
- Feature `risk1` will be a target feature
"""

df1['risk1'] = np.where(df1['risk']=='Bad Risk',1,0)
df1['risk1'].value_counts()/len(df1)*100

"""## Home_ownership"""

df1.home_ownership.value_counts()

df1.home_ownership.replace('NONE','OTHER',inplace=True)
df1.home_ownership.replace('ANY','OTHER',inplace=True)

"""# Data Visualization - Analisis Multivariate

## Purpose vs Risk
"""

plt.figure(figsize=(6, 5))
ax = sns.countplot(y="purpose",hue='risk',palette='GnBu_r',order=df1['purpose'].value_counts().index, data=df1)
plt.legend(loc=4)
ax.set(title="Purpose vs Risk");

"""## Term vs Risk"""

plt.figure(figsize=(6, 5))
ax = sns.countplot(x="term",hue='risk',palette='GnBu_r', data=df1)

"""## Home owner_ship vs Risk"""

plt.figure(figsize=(6, 5))
ax = sns.countplot(x="home_ownership",hue='risk',palette='GnBu_r',order=df1['home_ownership'].value_counts().index, data=df1)

"""## Lama bekerja vs Risk"""

plt.figure(figsize=(15, 5))
ax = sns.countplot(x="emp_length",hue='risk', order =['< 1 year','1 year','2 years','3 years','4 years','5 years','6 years','7 years','8 years',
                                                      '9 years','10+ years'], palette='GnBu_r', data=df1)

"""## Jumlah Loan vs Year"""

plt.figure(figsize=(10, 5))
ax = sns.boxplot(x="tahun_pinjaman",y='loan_amnt',palette='GnBu_r', data=df1)

"""## Jumlah Loan vs Jumlah peminjam vs year"""

df1_loan = df1.groupby('tahun_pinjaman').agg({'loan_amnt':'sum'}).sort_values('tahun_pinjaman',ascending = True)
df1_loan=df1_loan.rename(columns ={'loan_amnt':'total_loan'}).reset_index()
df1_loan

df1_loan1 = df1.groupby('tahun_pinjaman').agg({'loan_amnt':'count'}).sort_values('tahun_pinjaman',ascending = True)
df1_loan1=df1_loan1.rename(columns ={'loan_amnt':'total_peminjam'}).reset_index()
df1_loan1

df1_loan2 = df1_loan.merge(df1_loan1,left_on=['tahun_pinjaman'],right_on=['tahun_pinjaman'], how ='left')
df1_loan2

figure, axes = plt.subplots(1, 2, sharex=True,
                            figsize=(15, 5))
sns.barplot(x="tahun_pinjaman", y="total_loan", data=df1_loan2,  orient='v' ,palette='GnBu', ax=axes[0])
sns.barplot(x="tahun_pinjaman", y="total_peminjam", data=df1_loan2,  orient='v' ,palette='GnBu', ax=axes[1]);

"""## Jumlah Loan vs Risk vs Year"""

plt.figure(figsize=(10, 5))
ax = sns.boxplot(x="tahun_pinjaman",y='loan_amnt', hue= 'risk', palette='GnBu_r', data=df1)

"""## Jumlah Loan vs Grade vs Risk"""

plt.figure(figsize=(10, 5))
ax = sns.boxplot(x="grade",y='loan_amnt', hue= 'risk', order =['A','B','C','D','E','F','G'],palette='GnBu_r', data=df1)
plt.legend(loc=1);

"""## Jumlah Loan vs Loan_status"""

plt.figure(figsize=(10, 5))
ax = sns.boxplot(y="loan_status",x='loan_amnt',palette='GnBu_r', data=df1)

"""## Jumlah Loan vs Int Rate"""

plt.figure(figsize=(10, 5))
ax = sns.boxplot(y="loan_status",x='int_rate',palette='GnBu_r', data=df1)

"""## DTI vs year"""

plt.figure(figsize=(10, 5))
ax = sns.boxplot(x="tahun_pinjaman",y='dti',palette='GnBu_r', data=df1)

"""## DTI vs risk vs year"""

plt.figure(figsize=(10, 5))
ax = sns.boxplot(x="tahun_pinjaman",y='dti', hue= 'risk', palette='GnBu_r', data=df1)

"""## DTI vs risk vs grade"""

plt.figure(figsize=(10, 5))
ax = sns.boxplot(x="grade",y='dti', hue= 'risk', order =['A','B','C','D','E','F','G'],palette='GnBu_r', data=df1)
plt.legend(loc=1);

"""## Int Rate vs Year"""

plt.figure(figsize=(10, 5))
ax = sns.boxplot(x="tahun_pinjaman",y='int_rate',palette='GnBu_r', data=df1)

plt.figure(figsize=(10, 5))
ax = sns.boxplot(x="tahun_pinjaman",y='int_rate',hue ='risk',palette='GnBu_r', data=df1)

"""## Duration Loan vs Risk"""

plt.figure(figsize=(10, 3))
ax = sns.boxplot(x='duration_loan',y='risk',palette='GnBu_r', data=df1)

"""## Heatmap"""

plt.figure(figsize=(20, 25))
sns.heatmap(df1.corr(), cmap = 'Blues', annot = True, fmt = '.2f');

"""https://medium.com/@achmad.rozie/ml-create-customer-churn-prediction-using-3-method-comparison-support-vector-machine-naive-c44d6a6f871a

## Dropping Feature part 4

To prevent multicolinearity : some feature should be omitted.
"""

df1=df1.drop(columns=['loan_amnt','funded_amnt','funded_amnt_inv','installment','total_pymnt','total_pymnt_inv',
                      'total_rec_int','out_prncp_inv'])

plt.figure(figsize=(20, 25))
sns.heatmap(df1.corr(), cmap = 'Blues', annot = True, fmt = '.2f');

"""# Data preprocessing Part II

## Handling Null Value
"""

total_null = df1.isnull().sum() # mencari null value masing-masing kolom
percent_missing = df1.isnull().sum() * 100/ len(df) # dicari persentase nya
dtypes = [df1[col].dtype for col in df1.columns] # buat kolom dtype = tipe data

df_missing_value = pd.DataFrame({'total_null': total_null, # buat judulnya dan diisi dari 3 varibel diatas
                                'data_type': dtypes,
                                'percent_missing': percent_missing})
df_missing_value.sort_values(['data_type','percent_missing'], ascending = False,inplace = True) # sorting berdasarkan tipe data dan persentase missing
missing_value = df_missing_value[(df_missing_value['percent_missing']>0)].reset_index() #filter yang percentnya diatas 0)
missing_value

"""Handling Null Value :
- Dropping value because high cardinality: `emp_title`
- Unnecessary feature : `last_pymnt_d`, and `year_last_payment`
- Duplication feature : `year`, and `issue_d`, already used for `duration_loan`, `risk` only for data visualization, `risk1` for modelling
- Imputation : `title` with mode
- Numeric value : Imputation with median value
- Imputation with 0 : `tot_coll_amt`,`tot_cur_bal`,`total_rev_hi_lim`, replace missing value with "0" because asumption that customer didn't borrowed

### Dropping feature part 5
"""

df1=df1.drop(columns=['emp_title','last_pymnt_d','year_last_payment'])

df1=df1.drop(columns=['year','issue_d','risk'])

for col in ['tot_coll_amt','tot_cur_bal','total_rev_hi_lim']:
    df1[col] = df1[col].fillna(0)

for col in df1.select_dtypes(exclude = 'object'):
    df1[col] = df1[col].fillna(df1[col].median())
df1.isnull().sum()

for col in df1.select_dtypes(include = 'object'):
    df1[col] = df1[col].fillna(df1[col].mode().iloc[0])
df1.isnull().sum()

"""## Extract Data Term
- `term` -> numerical 36 and 60
"""

df1['term'] = df1['term'].apply(lambda term: int(term[:3]))

df1.info()

"""## Handling Outlier"""

from scipy import stats

print(f'Jumlah baris sebelum memfilter outlier: {len(df1)}')

filtered_entries = np.array([True] * len(df1))

for col in df1.select_dtypes(exclude = 'object'):
    zscore = abs(stats.zscore(df1[col])) # hitung absolute z-scorenya
    filtered_entries = (zscore < 3) & filtered_entries # keep yang kurang dari 3 absolute z-scorenya
    
df1 = df1[filtered_entries] # filter, cuma ambil yang z-scorenya dibawah 3

print(f'Jumlah baris setelah memfilter outlier: {len(df1)}')

plt.figure(figsize=(20, 20))
sns.heatmap(df1.corr(), cmap = 'Blues', annot = True, fmt = '.2f');

"""## Dropping feature part 6

Dropping value again for `collections_12_mths_ex_med`,`acc_now_delinq`, because only 1 unique value after handling outlier
"""

df1=df1.drop(columns=['collections_12_mths_ex_med','acc_now_delinq'])

df1=df1.drop(columns=['title','addr_state','loan_status']) # high cardinality again, already use for only Data Visualization

df1.info()

"""# Feature Selection Using Weight of Evidence & Information Value

[Source](https://github.com/okyhariawan/VIX-IDX-Credit-Risk-Loan-Prediction/blob/main/ID_X_Partners_Credit_Risk_Loan_Prediction.ipynb)
"""

df2= df1.copy()

# code automation

def woe(raw, feature_name):
    # probability analysis
    feature_name = raw.groupby(feature_name).agg(num_observation=('risk1','count'),
                                                good_loan_prob=('risk1','mean')).reset_index()
    
    # find the feature proportion
    feature_name['feat_proportion'] = feature_name['num_observation']/(feature_name['num_observation'].sum())
    
    # find number of approved loan behavior
    feature_name['num_loan_approve'] = feature_name['feat_proportion'] * feature_name['num_observation']

    # find number of declined loan behavior
    feature_name['num_loan_decline'] = (1-feature_name['feat_proportion']) * feature_name['num_observation']

    # find approved loan proportion
    feature_name['prop_loan_approve'] = feature_name['num_loan_approve'] / (feature_name['num_loan_approve'].sum())

    # find declined loan proportion
    feature_name['prop_loan_decline'] = feature_name['num_loan_decline'] / (feature_name['num_loan_decline'].sum())

    # calculate weight of evidence
    feature_name['weight_of_evidence'] = np.log(feature_name['prop_loan_approve'] / feature_name['prop_loan_decline'])

    # sort values by weight of evidence
    feature_name = feature_name.sort_values('weight_of_evidence').reset_index(drop=True)
    
    # calculate information value
    feature_name['information_value'] = (feature_name['prop_loan_approve']-feature_name['prop_loan_decline']) * feature_name['weight_of_evidence']
    feature_name['information_value'] = feature_name['information_value'].sum()

    #Show
    feature_name = feature_name.drop(['feat_proportion','num_loan_approve','num_loan_decline','prop_loan_approve','prop_loan_decline'],axis = 1)

    return feature_name

"""## Data Categorik"""

# Categorical Statistic Value
df2.describe(include = 'O').T

woe(df2,'grade')

woe(df2,'emp_length')

woe(df2,'home_ownership')

woe(df2,'verification_status')

woe(df2,'purpose')

woe(df2,'initial_list_status')

df2.describe().T

"""## Data Numerik"""

woe(df2,'term')

#refining class = 10 class
df2['int_rate_woe'] = pd.cut(df2['int_rate'], 10)
woe(df2,'int_rate_woe')

#refining class = 10 class
df2['annual_inc_woe'] = pd.cut(df2['annual_inc'], 10)
woe(df2,'annual_inc_woe')

#refining class = 10 class
df2['dti_woe'] = pd.cut(df2['dti'], 10)
woe(df2,'dti_woe')

woe(df2,'delinq_2yrs')

woe(df2,'inq_last_6mths')

#refining class = 10 class
df2['open_acc_woe'] = pd.cut(df2['open_acc'], 5)
woe(df2,'open_acc_woe')

woe(df2,'pub_rec')

#refining class = 10 class
df2['revol_bal_woe'] = pd.cut(df2['revol_bal'], 10)
woe(df2,'revol_bal_woe')

#refining class = 10 class
df2['revol_util_woe'] = pd.cut(df2['revol_util'], 10)
woe(df2,'revol_util_woe')

#refining class = 10 class
df2['total_acc_woe'] = pd.cut(df2['total_acc'], 10)
woe(df2,'total_acc_woe')

#refining class = 10 class
df2['out_prncp_woe'] = pd.cut(df2['out_prncp'], 10)
woe(df2,'out_prncp_woe')

#refining class = 10 class
df2['total_rec_prncp_woe'] = pd.cut(df2['total_rec_prncp'], 10)
woe(df2,'total_rec_prncp_woe')

#refining class = 5 class
df2['total_rec_late_fee_woe'] = pd.cut(df2['collection_recovery_fee'], 5)
woe(df2,'total_rec_late_fee_woe')

#refining class = 10 class
df2['recoveries_woe'] = pd.cut(df2['recoveries'], 10)
woe(df2,'recoveries_woe')

#refining class = 10 class
df2['collection_recovery_fee_woe'] = pd.cut(df2['collection_recovery_fee'], 10)
woe(df2,'collection_recovery_fee_woe')

#refining class = 10 class
df2['last_pymnt_amnt_woe'] = pd.cut(df2['last_pymnt_amnt'], 10)
woe(df2,'last_pymnt_amnt_woe')

#refining class = 10 class
df2['tot_coll_amt_woe'] = pd.cut(df2['tot_coll_amt'], 10)
woe(df2,'tot_coll_amt_woe')

#refining class = 10 class
df2['tot_cur_bal_woe'] = pd.cut(df2['tot_cur_bal'], 10)
woe(df2,'tot_cur_bal_woe')

#refining class = 10 class
df2['total_rev_hi_lim_woe'] = pd.cut(df2['total_rev_hi_lim'], 10)
woe(df2,'total_rev_hi_lim_woe')

#refining class = 10 class
df2['duration_loan_woe'] = pd.cut(df2['duration_loan'],10)
woe(df2,'duration_loan_woe')

woe(df2,'tahun_pinjaman')

"""## Summary 

Information value < 0.02 (useless predictive),
Information value > 0.5 (suspicious predictive), we collect IV 0.2-0.5
"""

drop_list = ['emp_length', 'verification_status', 'purpose','term','delinq_2yrs',
             'inq_last_6mths','open_acc','pub_rec','revol_util','out_prncp','total_rec_late_fee','recoveries','collection_recovery_fee',
             'last_pymnt_amnt','tot_coll_amt','tot_cur_bal','total_rev_hi_lim','tahun_pinjaman']
print('number of features that we will drop :',len(drop_list))

df_model = df1.copy()

"""## Dropping feature part 7"""

df_model.drop(['emp_length', 'verification_status', 'purpose','term','delinq_2yrs',
               'inq_last_6mths','open_acc','pub_rec','revol_util','out_prncp','total_rec_late_fee','recoveries','collection_recovery_fee',
               'last_pymnt_amnt','tot_coll_amt','tot_cur_bal','total_rev_hi_lim','tahun_pinjaman'], axis = 1, inplace=True)

df_model.info()

"""# Feature Engineering (III) for modelling"""

df_encode=df_model.copy()

"""## Feature Encoding for Categorical"""

one_hot_cats=df_encode[['grade','home_ownership','initial_list_status']]
one_hot=pd.get_dummies(one_hot_cats)

one_hot.info()

df_encode=df_encode.join(one_hot)

df_encode=df_encode.drop(columns=['grade','home_ownership','initial_list_status'])

df_encode.info()

"""## Feature scaler for Numerical """

from sklearn.preprocessing import MinMaxScaler, StandardScaler

# semua kolom kita standarisasi (karena sudah mendekati distribusi normal)

df_encode['int_rate_norm'] = StandardScaler().fit_transform(df_encode['int_rate'].values.reshape(len(df_encode),1))
df_encode['annual_inc_norm'] = StandardScaler().fit_transform(df_encode['annual_inc'].values.reshape(len(df_encode),1))
df_encode['dti_norm'] = StandardScaler().fit_transform(df_encode['dti'].values.reshape(len(df_encode),1))
df_encode['revol_bal_norm'] = StandardScaler().fit_transform(df_encode['revol_bal'].values.reshape(len(df_encode),1))
df_encode['total_acc_norm'] = StandardScaler().fit_transform(df_encode['total_acc'].values.reshape(len(df_encode),1))
df_encode['total_rec_prncp_norm'] = StandardScaler().fit_transform(df_encode['total_rec_prncp'].values.reshape(len(df_encode),1))
df_encode['duration_loan_norm'] = StandardScaler().fit_transform(df_encode['duration_loan'].values.reshape(len(df_encode),1))

df_encode=df_encode.drop(columns=['int_rate','annual_inc','dti','revol_bal','total_acc','total_rec_prncp','duration_loan'])

df_encode.info()

"""# Modelling (Before Handle Class Imbalance)"""

X = df_encode.drop(labels=['risk1'],axis=1)
y = df_encode[['risk1']]

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,stratify=y,random_state = 42)

y_train.value_counts()

from datetime import datetime as dt
from collections import defaultdict
from xgboost import XGBClassifier
import lightgbm as lgb
import time
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score, recall_score, precision_score,roc_auc_score, roc_curve, f1_score
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay

knn = KNeighborsClassifier()
xgb = XGBClassifier()
rf = RandomForestClassifier()
grad = GradientBoostingClassifier()
LGBM = lgb.LGBMClassifier()

def eval_classification(model, xtrain, ytrain, xtest, ytest):
    ypred = model.predict(xtest)
    print("Accuracy (Test Set): %.2f" % accuracy_score(ytest, ypred))
    print("Precision (Test Set): %.2f" % precision_score(ytest, ypred))
    print("Recall (Test Set): %.2f" % recall_score(ytest, ypred))
    print("F1-Score (Test Set): %.2f" % f1_score(ytest, ypred))
    
    y_pred_proba = model.predict_proba(xtest)
    print("AUC: %.2f" % roc_auc_score(ytest, y_pred_proba[:, 1]))
    
    confusion_matrix = metrics.confusion_matrix(ytest, ypred)
    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['good', 'bad'])
    cm_display.plot()
    plt.show()

"""### KNeighborsClassifier"""

knn.fit(X_train,y_train)
eval_classification(knn,X_train,y_train,X_test,y_test)

print('Train score: ' + str(knn.score(X_train, y_train))) #accuracy
print('Test score: ' + str(knn.score(X_test, y_test))) #accuracy

"""### XGBClassifier"""

xgb.fit(X_train,y_train)
eval_classification(xgb,X_train,y_train,X_test,y_test)

print('Train score: ' + str(xgb.score(X_train, y_train))) #accuracy
print('Test score: ' + str(xgb.score(X_test, y_test))) #accuracy

"""### RandomForestClassifier"""

rf.fit(X_train,y_train)
eval_classification(rf,X_train,y_train,X_test,y_test)

print('Train score: ' + str(rf.score(X_train, y_train))) #accuracy
print('Test score: ' + str(rf.score(X_test, y_test))) #accuracy

"""### GradientBoostingClassifier"""

grad.fit(X_train,y_train)
eval_classification(grad,X_train,y_train,X_test,y_test)

print('Train score: ' + str(grad.score(X_train, y_train))) #accuracy
print('Test score: ' + str(grad.score(X_test, y_test))) #accuracy

"""### LGBMClassifier"""

LGBM.fit(X_train,y_train)
eval_classification(LGBM,X_train,y_train,X_test,y_test)

print('Train score: ' + str(LGBM.score(X_train, y_train))) #accuracy
print('Test score: ' + str(LGBM.score(X_test, y_test))) #accuracy

"""Source : https://github.com/okyhariawan/VIX-IDX-Credit-Risk-Loan-Prediction

# Handling Class Imbalance ( Under sampling )
"""

from imblearn.under_sampling import RandomUnderSampler
# Randomly under sample the majority class
rus = RandomUnderSampler(random_state=42)
X_train_rus, y_train_rus= rus.fit_resample(X_train, y_train)
# Check the number of records after under sampling

y_train_rus.value_counts()

"""### KNeighborsClassifier"""

knn.fit(X_train_rus,y_train_rus)
eval_classification(knn,X_train_rus,y_train_rus,X_test,y_test)

print('Train score: ' + str(knn.score(X_train_rus, y_train_rus))) #accuracy
print('Test score: ' + str(knn.score(X_test, y_test))) #accuracy

"""### XGBClassifier"""

xgb.fit(X_train_rus,y_train_rus)
eval_classification(xgb,X_train_rus,y_train_rus,X_test,y_test)

print('Train score: ' + str(xgb.score(X_train_rus, y_train_rus))) #accuracy
print('Test score: ' + str(xgb.score(X_test, y_test))) #accuracy

"""### RandomForestClassifier"""

rf.fit(X_train_rus,y_train_rus)
eval_classification(rf,X_train_rus,y_train_rus,X_test,y_test)

print('Train score: ' + str(rf.score(X_train_rus, y_train_rus))) #accuracy
print('Test score: ' + str(rf.score(X_test, y_test))) #accuracy

"""### GradientBoostingClassifier



"""

grad.fit(X_train_rus,y_train_rus)
eval_classification(grad,X_train_rus,y_train_rus,X_test,y_test)

print('Train score: ' + str(grad.score(X_train_rus, y_train_rus))) #accuracy
print('Test score: ' + str(grad.score(X_test, y_test))) #accuracy

"""### LGBMClassifier"""

LGBM.fit(X_train_rus,y_train_rus)
eval_classification(LGBM,X_train_rus,y_train_rus,X_test,y_test)

print('Train score: ' + str(LGBM.score(X_train_rus, y_train_rus))) #accuracy
print('Test score: ' + str(LGBM.score(X_test, y_test))) #accuracy

"""# Feature Importance"""

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(10).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

show_feature_importance(LGBM)

"""- int_rate : bunga pinjaman
- annual_inc : pendapatan tahunan
- dti : debt to income ratio
- revol_bal : total saldo revolving credit
- total_acc : total credit saat ini
- total_rec_prncp : total uang yang diterima
- duration_loan : durasi peminjaman (bulan)
- risk1 : good / bad risk
- grade : grade
- home_ownership : kepemilikan rumah
- initial_list_status : Status daftar awal pinjaman. Nilai yang mungkin adalah - utuh, fraksional
"""